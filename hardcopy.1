 69             self.src_word_emb = nn.Embedding(
 70                 n_src_vocab, d_word_vec, padding_idx=Constants.PAD)
 71
 72         self.position_enc = nn.Embedding.from_pretrained(
 73             get_sinusoid_encoding_table(n_position, d_word_vec, padding_idx=0),
 74             freeze=True)
 75
 76         self.layer_stack = nn.ModuleList([
 77             EncoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)
 78             for _ in range(n_layers)])
 79
 80     def forward(self, src_seq, src_pos, return_attns=False):
 81
 82         enc_slf_attn_list = []
 83
 84         # -- Prepare masks
 85         slf_attn_mask = get_attn_key_pad_mask(seq_k=src_seq, seq_q=src_seq)
 86         non_pad_mask = get_non_pad_mask(src_seq)
 87
 88         # -- Forward
 89         enc_output = self.src_word_emb(src_seq) + self.position_enc(src_pos)
 90
 91         for enc_layer in self.layer_stack:
 92             enc_output, enc_slf_attn = enc_layer(
 93                 enc_output,
 94                 non_pad_mask=non_pad_mask,
 95                 slf_attn_mask=slf_attn_mask)
 96             if return_attns:
 97                 enc_slf_attn_list += [enc_slf_attn]
 98
 99         if return_attns:
100             return enc_output, enc_slf_attn_list
101         return enc_output,
102
103 class Decoder(nn.Module):
104     ''' A decoder model with self attention mechanism. '''
105
106     def __init__(
107             self,
dyfyphkustqiong@instance-1:~/qa-kaggle$ python model.py
> /home/dyfyphkustqiong/qa-kaggle/model.py(43)<module>()
-> print(batch)
(Pdb)
